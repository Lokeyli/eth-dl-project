{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.14.0 / PyTorch 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys,os,os.path\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents, ClientManager\n",
    "from flwr.server.strategy import Strategy, FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context\n",
    "from flwr.common import FitRes, Parameters, parameters_to_ndarrays\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common.logger import set_logger_propagation\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device(device)  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {fl.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the above ugly code of flower client into a class\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "class TaskType(Enum):\n",
    "\n",
    "    CLASSFICATION = 0\n",
    "    REGRESSION = 1\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def train(net, trainloader, epochs: int, verbose=False, device = \"cpu\", task_type = TaskType.CLASSFICATION):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    if task_type == TaskType.CLASSFICATION:    \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    elif task_type == TaskType.REGRESSION:\n",
    "        criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            if task_type == TaskType.REGRESSION:\n",
    "                outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            if task_type == TaskType.CLASSFICATION:\n",
    "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        if verbose:\n",
    "            if task_type == TaskType.CLASSFICATION:\n",
    "                epoch_acc = correct / total\n",
    "                print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "            elif task_type == TaskType.REGRESSION:\n",
    "                print(f\"Epoch {epoch+1}: train loss {epoch_loss}\")\n",
    "\n",
    "def test(net, testloader, device = \"cpu\"):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "def test_regression(net, testloader, device=\"cpu\"):\n",
    "    \"\"\"Evaluate the regression model on the entire test set.\"\"\"\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    sum_of_squares, total_samples = 0.0, 0\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = net(x).squeeze() # NOTE: You maight want to modify this part\n",
    "            sum_of_squares += criterion(outputs, y).item()\n",
    "            total_samples += len(y)\n",
    "\n",
    "    if total_samples > 0:\n",
    "        avg_mse = sum_of_squares / total_samples\n",
    "    else:\n",
    "        avg_mse = 0.0\n",
    "    avg_loss = avg_mse\n",
    "\n",
    "    # Note that to make sure the consistence,\n",
    "    # we return mse twice to match {loss, accurancy as the test function}\n",
    "    return avg_loss, avg_mse ** 0.5\n",
    "\n",
    "# Custom Client Class\n",
    "class FLClient(NumPyClient):\n",
    "    \"\"\"A Flower client that holds its own model and training data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        device: torch.device,\n",
    "        client_id: int,\n",
    "        epochs: int = 1,\n",
    "        task_type: TaskType = TaskType.CLASSFICATION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.epochs = epochs\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> List[np.ndarray]:\n",
    "        \"\"\"Return the current local model parameters.\"\"\"\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, device=self.device, epochs=self.epochs, verbose=False, task_type = self.task_type)\n",
    "        new_params = get_parameters(self.net)\n",
    "        # Return partition-id in the metrics\n",
    "        # The simplest way to store the model\n",
    "        return new_params, len(self.trainloader.dataset), {\"partition-id\": self.client_id}\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        if self.task_type == TaskType.CLASSFICATION:\n",
    "            loss, accuracy = test(self.net, self.valloader, self.device)\n",
    "            print(f\"[Client {self.client_id}] Evaluate -> Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            return float(loss), len(self.valloader.dataset), {\"accuracy\": float(accuracy)}\n",
    "        elif self.task_type == TaskType.REGRESSION:\n",
    "            loss, mse = test_regression(self.net, self.valloader, self.device)\n",
    "            print(f\"[Client {self.client_id}] Evaluate -> Loss: {mse:.4f}\")\n",
    "            return float(loss), len(self.valloader.dataset), {\"MSE\": float(mse)}\n",
    "\n",
    "# Custom Client for House Pricing Dataset\n",
    "\n",
    "class HousePricingClient(fl.client.NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        client_id: int,\n",
    "        epochs: int = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> List[np.ndarray]:\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    '''def set_parameters(self, parameters):\n",
    "        state_dict = dict(zip(self.model.state_dict().keys(), parameters))\n",
    "        self.model.load_state_dict({k: torch.tensor(v) for k, v in state_dict.items()})'''\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.train_loader, device=self.device, epochs=self.epochs, verbose=False)\n",
    "        new_params = get_parameters(self.net)\n",
    "        # Return partition-id in the metrics\n",
    "        # The simplest way to store the model\n",
    "        return new_params, len(self.train_loader.dataset), {\"partition-id\": self.client_id}\n",
    "        \n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, rmse = test_regression(self.net, self.val_loader, self.device)\n",
    "        print(f\"[Client {self.client_id}] Evaluate -> Loss: {loss:.4f}, RMSE: {rmse:.4f}\")\n",
    "        return float(loss), len(self.val_loader.dataset), {\"RMSE\": float(rmse)}\n",
    "\n",
    "\n",
    "class DefaultStrategy(FedAvg):\n",
    "\n",
    "    # A custom strategy to store all the parameters.\n",
    "    # https://github.com/adap/flower/issues/487\n",
    "    # https://flower.ai/docs/framework/how-to-save-and-load-model-checkpoints.html\n",
    "\n",
    "    def __init__(self, model: type, total_round: int, only_last: bool = True, save_dir: str = \"models\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.model = model\n",
    "        self.total_round = total_round\n",
    "        self.only_last = only_last\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: list[tuple[ClientProxy, FitRes]],\n",
    "        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:\n",
    "        \"\"\"\n",
    "        Aggregate model weights using weighted average.\n",
    "        Also save each client's model and the global server model.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.only_last and server_round < self.total_round:\n",
    "            return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "\n",
    "        # For each client which returned FitRes, save the client model\n",
    "        for (_, fit_res) in results:\n",
    "            id_ = fit_res.metrics[\"partition-id\"]\n",
    "\n",
    "            client_parameters: Optional[Parameters] = fit_res.parameters\n",
    "            if client_parameters is not None:\n",
    "                net = self.model()\n",
    "                print(f\"[Round {server_round}] Saving model for client {id_}...\")\n",
    "\n",
    "                # Convert `Parameters` to `list[np.ndarray]`\n",
    "                client_ndarrays : list[np.ndarray] = parameters_to_ndarrays(\n",
    "                    client_parameters\n",
    "                )\n",
    "\n",
    "                # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "                params_dict = zip(net.state_dict().keys(), client_ndarrays)\n",
    "                state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "                net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "                # Save the model to disk\n",
    "                torch.save(net.state_dict(), f\"{self.save_dir}/client-{server_round}-{id_}.pth\")\n",
    "\n",
    "        # If `aggregated_parameters` is not None, update the global net and save it\n",
    "        if aggregated_parameters is not None:\n",
    "            net = self.model()\n",
    "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "            # Convert `Parameters` to `list[np.ndarray]`\n",
    "            aggregated_ndarrays: list[np.ndarray] = parameters_to_ndarrays(\n",
    "                aggregated_parameters\n",
    "            )\n",
    "\n",
    "            # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "            params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Save the model to disk\n",
    "            torch.save(net.state_dict(), f\"{self.save_dir}/server-{server_round}.pth\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following class to run the experiment\n",
    "\n",
    "# You need to provide the following information:\n",
    "# 1. The Network class (dont instantiate it)\n",
    "#       (assume we use the same network for all clients and server)\n",
    "# 2. The list of data loaders for each client,\n",
    "#       where loaders is a list of loader tuples (train, val, test)\n",
    "#       i.e. loaders = [ (train_loader_0, val_loader_0, test_loader_0), ... ]\n",
    "#       NOTE: In fit and evaluate, we ONLY use the train_loader and val_loader,\n",
    "#             But we ask you to pyt them together for simplicity for any future test use.\n",
    "#       NOTE: we assume the number of clients == number of data loaders\n",
    "# 3. Number of clients\n",
    "\n",
    "# See next block for an example of how to use this class\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "set_logger_propagation(logger, True)\n",
    "\n",
    "class FLExperiment:\n",
    "    \"\"\"\n",
    "    A federated learning experiment interface class.\n",
    "\n",
    "    NOTE: For each client, we now expect a tuple of three DataLoaders:\n",
    "    (train_loader, val_loader, test_loader).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_cls: type,\n",
    "        client_loaders: List[Tuple[DataLoader, DataLoader, DataLoader]],\n",
    "        num_clients: int,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        local_epochs: int = 1,\n",
    "        num_rounds: int = 5,\n",
    "        task_type: TaskType = TaskType.REGRESSION,\n",
    "        # strategy: Optional[Strategy] = None, # Is not supported yet. and may not be needed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_cls (type): A PyTorch nn.Module class (not an instance).\n",
    "                We'll instantiate `model_cls()` for each client and server.\n",
    "            client_loaders (List[(DataLoader, DataLoader, DataLoader)]):\n",
    "                A list of (train_loader, val_loader, test_loader) for each client.\n",
    "            num_clients (int): Number of clients to simulate.\n",
    "            device (torch.device): CPU or GPU device.\n",
    "            local_epochs (int): Local epochs on each client per round.\n",
    "            num_rounds (int): How many global training rounds.\n",
    "            strategy (Optional[Strategy]): Use a custom Flower strategy or fallback to default FedAvg.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing FLExperiment\")\n",
    "        if len(client_loaders) != num_clients:\n",
    "            raise ValueError(\n",
    "                f\"Number of client loader tuples ({len(client_loaders)}) does not match \"\n",
    "                f\"the number of clients ({num_clients}).\"\n",
    "            )\n",
    "\n",
    "        self.model_cls = model_cls\n",
    "        self.client_loaders = client_loaders\n",
    "        self.num_clients = num_clients\n",
    "        self.local_epochs = local_epochs\n",
    "        self.num_rounds = num_rounds\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "\n",
    "        # Store final trained models\n",
    "        self._client_models: List[Optional[nn.Module]] = [None] * self.num_clients\n",
    "        self._server_model: Optional[nn.Module] = None\n",
    "\n",
    "        # Create one model per client (instantiate model_cls)\n",
    "        self.client_nets = [self.model_cls().to(self.device) for _ in range(self.num_clients)]\n",
    "\n",
    "        self.strategy = self._create_default_strategy(save_only_last=True)\n",
    "        logger.info(\"FLExperiment initialized successfully\")\n",
    "        # # Use user-provided strategy or create a default one\n",
    "        # if strategy is None:\n",
    "        #     self.strategy = self._create_default_strategy()\n",
    "        # else:\n",
    "        #     self.strategy = strategy\n",
    "\n",
    "    def _create_default_strategy(self, save_only_last: bool) -> Strategy:\n",
    "        \"\"\"Create a default FedAvg strategy with a minimal server_evaluate.\"\"\"\n",
    "\n",
    "        logger.debug(\"Creating default strategy\")\n",
    "\n",
    "        def server_evaluate(\n",
    "            server_round: int,\n",
    "            parameters: NDArrays,\n",
    "            config: Dict[str, Scalar]\n",
    "        ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "            # Minimal server eval (no real evaluation)\n",
    "            net = self.model_cls().to(self.device)\n",
    "            set_parameters(net, parameters)\n",
    "            print(f\"[Server] Round {server_round} - no global evaluation implemented.\")\n",
    "            return None\n",
    "        \n",
    "        def weighted_average(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
    "            accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "            examples = [num_examples for num_examples, _ in metrics]\n",
    "            if sum(examples) == 0:\n",
    "                return {\"accuracy\": 0.0}\n",
    "            return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "        def weighted_average_regression(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
    "            total_sum_of_squares = 0.0\n",
    "            total_samples = 0\n",
    "            for (num_examples, m) in metrics:\n",
    "                if \"sum_of_squares\" in m:\n",
    "                    total_sum_of_squares += m[\"sum_of_squares\"]\n",
    "                    total_samples += num_examples\n",
    "            if total_samples == 0:\n",
    "                return {\"rmse\": 0.0}\n",
    "            rmse = (total_sum_of_squares / total_samples) ** 0.5\n",
    "            return {\"rmse\": rmse}\n",
    "        \n",
    "        if self.task_type == TaskType.CLASSFICATION:\n",
    "            aggregation_fn = weighted_average\n",
    "        else:\n",
    "            aggregation_fn = weighted_average_regression\n",
    "\n",
    "        default_strategy = DefaultStrategy(\n",
    "            model = self.model_cls,\n",
    "            total_round = self.num_rounds,\n",
    "            only_last = True,\n",
    "            fraction_fit=1.0,\n",
    "            fraction_evaluate=1.0,\n",
    "            min_fit_clients=self.num_clients,\n",
    "            min_evaluate_clients=self.num_clients,\n",
    "            min_available_clients=self.num_clients,\n",
    "            evaluate_fn=server_evaluate,\n",
    "            evaluate_metrics_aggregation_fn=aggregation_fn,\n",
    "        )\n",
    "        return default_strategy\n",
    "\n",
    "    def _client_fn(self, context: Context) -> Client:\n",
    "        \"\"\"Construct one Flower client using the partition_id to pick (train, val, test).\"\"\"\n",
    "        partition_id = context.node_config[\"partition-id\"]\n",
    "        trainloader, valloader, testloader = self.client_loaders[partition_id]\n",
    "        net = self.client_nets[partition_id]\n",
    "        logger.info(f\"Creating client {partition_id}\")\n",
    "\n",
    "        client = HousePricingClient(\n",
    "            net=net,\n",
    "            train_loader=trainloader,\n",
    "            val_loader=valloader,\n",
    "            test_loader = testloader,\n",
    "            device=self.device,\n",
    "            client_id=partition_id,\n",
    "            epochs=self.local_epochs\n",
    "        )\n",
    "        return client.to_client()\n",
    "\n",
    "    def _server_fn(self, context: Context) -> ServerAppComponents:\n",
    "        \"\"\"Server-side: configure strategy and server config.\"\"\"\n",
    "        config = ServerConfig(num_rounds=self.num_rounds)\n",
    "        return ServerAppComponents(strategy=self.strategy, config=config)\n",
    "\n",
    "    def run(self, save_only_last: bool = True) -> None:\n",
    "        \"\"\"Run the federated learning simulation and store final client/server models.\n",
    "        \n",
    "        Args:\n",
    "            save_only_last (bool): Save only the last round of models.\n",
    "                Default True. If False, all models will be saved.\n",
    "        \"\"\"\n",
    "        print(\"[FLExperiment] Starting federated training...\")\n",
    "        self.strategy = self._create_default_strategy(save_only_last=save_only_last)\n",
    "        client_app = ClientApp(client_fn=self._client_fn)\n",
    "        server_app = ServerApp(server_fn=self._server_fn)\n",
    "\n",
    "        # Resource allocation\n",
    "        if self.device.type == \"cuda\":\n",
    "            backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "        else:\n",
    "            backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "        # Run the simulation\n",
    "        run_simulation(\n",
    "            client_app=client_app,\n",
    "            server_app=server_app,\n",
    "            num_supernodes=self.num_clients,\n",
    "            backend_config=backend_config,\n",
    "        )\n",
    "        print(\"[FLExperiment] Federated training finished.\")\n",
    "\n",
    "    def get_clients(self, round_num: int = 0) -> List[nn.Module]:\n",
    "        \"\"\"Return final trained models for all clients (if they have been saved).\n",
    "        \n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            List[nn.Module]: List of final trained models for all clients.\n",
    "                The index of the list corresponds to the client ID \n",
    "                and the index of dataloader. \n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            return [\n",
    "                torch.load(f\"models/client-{round_num}-{cid}.pth\", map_location=self.device, weights_only=True)\n",
    "                for cid in range(self.num_clients)\n",
    "            ]\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Client models are not available. Have you called run() or set only_last=True?\")\n",
    "    \n",
    "    def get_client_dataloader_tuples(self, round_num: int = 0) -> List[Tuple[nn.Module, Tuple[DataLoader, DataLoader, DataLoader]]]:\n",
    "        \"\"\"Return the dataloaders for all clients.\n",
    "         \n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[nn.Module, Tuple[DataLoader, DataLoader, DataLoader]]]:\n",
    "                List of (client_model, (train_loader, val_loader, test_loader))\n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            clients = self.get_clients(round_num)\n",
    "            return list(zip(clients, self.client_loaders)) \n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Client dataloaders are not available. Have you called run() or set only_last=True?\")\n",
    "\n",
    "    def get_server(self, round_num: int = 0) -> nn.Module:\n",
    "        \"\"\"Return the final server model (if stored).\n",
    "\n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The final server model.\n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            return torch.load(f\"models/server-{round_num}.pth\", map_location=self.device, weights_only=True)\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Server model is not available. Have you called run() or set only_last=True?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Neural Network for Tabular Data\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size = 8):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders for House Pricing Dataset\n",
    "\n",
    "def load_data(client_id):\n",
    "    data_path = f\"house_pricing_datasets/client_{client_id}.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Encode categorical features\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    # Separate features and target\n",
    "    features = df.drop(columns=[\"House Price\"]).values\n",
    "    target = df[\"House Price\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(features))\n",
    "    val_size = int(0.1 * len(features))\n",
    "    test_size = len(features) - train_size - val_size\n",
    "    indices = np.random.permutation(len(features))\n",
    "    train_idx, val_idx, test_idx = indices[:train_size], indices[train_size:train_size + val_size], indices[train_size + val_size:]\n",
    "\n",
    "    X_train, y_train = features[train_idx], target[train_idx]\n",
    "    X_val, y_val = features[val_idx], target[val_idx]\n",
    "    X_test, y_test = features[test_idx], target[test_idx]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loaders for all clients\n",
    "NUM_CLIENTS = 5\n",
    "loaders = [\n",
    "    load_data(client_id) for client_id in range(1, NUM_CLIENTS + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 13:22:38,409 - INFO - Initializing FLExperiment\n",
      "2025-01-09 13:22:38,415 - DEBUG - Creating default strategy\n",
      "2025-01-09 13:22:38,419 - INFO - FLExperiment initialized successfully\n",
      "2025-01-09 13:22:38,421 - DEBUG - Creating default strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FLExperiment] Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 0 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 1 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m [Client 1] Evaluate -> Loss: 20749966.0000, RMSE: 4555.2131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 2 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 3 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 4 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 5 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m [Client 4] Evaluate -> Loss: 60171427.5556, RMSE: 7757.0244\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 6 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=45884)\u001b[0m c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=45884)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 7 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 8 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 9 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 10 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 11]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m [Client 2] Evaluate -> Loss: 12045480.0000, RMSE: 3470.6599\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 11 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 12]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 12 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 13]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 13 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 14]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 14 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=20172)\u001b[0m c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=20172)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 15]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 15 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 16]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m [Client 2] Evaluate -> Loss: 12045480.0000, RMSE: 3470.6599\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 16 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 17]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 17 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 18]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 18 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 19]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 19 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m [Client 2] Evaluate -> Loss: 12045480.0000, RMSE: 3470.6599\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 20] Saving model for client 0...\n",
      "[Round 20] Saving model for client 1...\n",
      "[Round 20] Saving model for client 4...\n",
      "[Round 20] Saving model for client 3...\n",
      "[Round 20] Saving model for client 2...\n",
      "Saving round 20 aggregated_parameters...\n",
      "[Server] Round 20 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 20 round(s) in 46.56s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 54644372.53731343\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 54644370.62686567\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 54644372.53731343\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 6: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 7: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 8: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 9: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 10: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 11: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 12: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 13: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 14: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 15: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 16: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 17: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 18: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 19: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 20: 54644370.14925373\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'rmse': [(1, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (2, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (3, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (4, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (5, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (6, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (7, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (8, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (9, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (10, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (11, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (12, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (13, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (14, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (15, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (16, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (17, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (18, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (19, 0.0),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (20, 0.0)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=26560)\u001b[0m \u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n",
      "\u001b[36m(ClientAppActor pid=45884)\u001b[0m c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[36m(ClientAppActor pid=45884)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=41516)\u001b[0m [Client 1] Evaluate -> Loss: 20749964.0000, RMSE: 4555.2128\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "[FLExperiment] Federated training finished.\n"
     ]
    }
   ],
   "source": [
    "fl_exp = FLExperiment(\n",
    "    model_cls=Net,\n",
    "    client_loaders=loaders,\n",
    "    num_clients=5,\n",
    "    num_rounds=20,\n",
    "    task_type = TaskType.REGRESSION\n",
    ")\n",
    "\n",
    "\n",
    "fl_exp.run(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 rmse: 121.39111776886217\n",
      "Client 0 rmse: 14735.803473173764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Test the client model 0 on the corresponding test set\n",
    "\n",
    "clients = fl_exp.get_clients()\n",
    "model0_1 = Net().to(DEVICE)\n",
    "model0_1.load_state_dict(clients[0])\n",
    "model0_1.eval()\n",
    "\n",
    "_, _, test_loader_1 = loaders[0]\n",
    "loss, rmse = test_regression(model0_1, test_loader_1)\n",
    "print(f\"Client 0 rmse: {rmse}\")\n",
    "\n",
    "# Or Alternatively\n",
    "client_dls = fl_exp.get_client_dataloader_tuples()\n",
    "model0_2 = Net().to(DEVICE)\n",
    "model0_2.load_state_dict(client_dls[0][0])\n",
    "model0_2.eval()\n",
    "\n",
    "_, _, test_loader_2 = client_dls[0][1]\n",
    "loss, rmse = test_regression(model0_2, test_loader_2)\n",
    "print(f\"Client 0 rmse: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
