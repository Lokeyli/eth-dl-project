{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-09 14:03:56,354\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.14.0 / PyTorch 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys,os,os.path\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents, ClientManager\n",
    "from flwr.server.strategy import Strategy, FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context\n",
    "from flwr.common import FitRes, Parameters, parameters_to_ndarrays\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common.logger import set_logger_propagation\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device(device)  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {fl.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the above ugly code of flower client into a class\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "class TaskType(Enum):\n",
    "\n",
    "    CLASSFICATION = 0\n",
    "    REGRESSION = 1\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def train(net, trainloader, epochs: int, verbose=False, device = \"cpu\", task_type = TaskType.CLASSFICATION):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    if task_type == TaskType.CLASSFICATION:    \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    elif task_type == TaskType.REGRESSION:\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            if task_type == TaskType.REGRESSION:\n",
    "                outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            if task_type == TaskType.CLASSFICATION:\n",
    "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        if verbose:\n",
    "            if task_type == TaskType.CLASSFICATION:\n",
    "                epoch_acc = correct / total\n",
    "                print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "            elif task_type == TaskType.REGRESSION:\n",
    "                print(f\"Epoch {epoch+1}: train loss {epoch_loss}\")\n",
    "\n",
    "def test(net, testloader, device = \"cpu\"):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "def test_regression(net, testloader, device=\"cpu\"):\n",
    "    \"\"\"Evaluate the regression model on the entire test set.\"\"\"\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    sum_of_squares, total_samples = 0.0, 0\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            target = y.view(-1)\n",
    "            outputs = net(x).view(-1) # NOTE: You maight want to modify this part\n",
    "            sum_of_squares += criterion(outputs, target).item()\n",
    "            total_samples += len(y)\n",
    "\n",
    "    if total_samples > 0:\n",
    "        avg_mse = sum_of_squares / total_samples\n",
    "    else:\n",
    "        avg_mse = 0.0\n",
    "    avg_loss = avg_mse\n",
    "\n",
    "    # Note that to make sure the consistence,\n",
    "    # we return mse twice to match {loss, accurancy as the test function}\n",
    "    return avg_loss, avg_mse ** 0.5\n",
    "\n",
    "# Custom Client Class\n",
    "class FLClient(NumPyClient):\n",
    "    \"\"\"A Flower client that holds its own model and training data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        device: torch.device,\n",
    "        client_id: int,\n",
    "        epochs: int = 1,\n",
    "        task_type: TaskType = TaskType.CLASSFICATION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.epochs = epochs\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> List[np.ndarray]:\n",
    "        \"\"\"Return the current local model parameters.\"\"\"\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, device=self.device, epochs=self.epochs, verbose=False, task_type = self.task_type)\n",
    "        new_params = get_parameters(self.net)\n",
    "        # Return partition-id in the metrics\n",
    "        # The simplest way to store the model\n",
    "        return new_params, len(self.trainloader.dataset), {\"partition-id\": self.client_id}\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        if self.task_type == TaskType.CLASSFICATION:\n",
    "            loss, accuracy = test(self.net, self.valloader, self.device)\n",
    "            print(f\"[Client {self.client_id}] Evaluate -> Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            return float(loss), len(self.valloader.dataset), {\"accuracy\": float(accuracy)}\n",
    "        elif self.task_type == TaskType.REGRESSION:\n",
    "            loss, mse = test_regression(self.net, self.valloader, self.device)\n",
    "            print(f\"[Client {self.client_id}] Evaluate -> Loss: {mse:.4f}\")\n",
    "            return float(loss), len(self.valloader.dataset), {\"MSE\": float(mse)}\n",
    "\n",
    "# Custom Client for House Pricing Dataset\n",
    "\n",
    "class HousePricingClient(fl.client.NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        client_id: int,\n",
    "        epochs: int = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> List[np.ndarray]:\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    '''def set_parameters(self, parameters):\n",
    "        state_dict = dict(zip(self.model.state_dict().keys(), parameters))\n",
    "        self.model.load_state_dict({k: torch.tensor(v) for k, v in state_dict.items()})'''\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.train_loader, device=self.device, epochs=self.epochs, verbose=False)\n",
    "        new_params = get_parameters(self.net)\n",
    "        # Return partition-id in the metrics\n",
    "        # The simplest way to store the model\n",
    "        return new_params, len(self.train_loader.dataset), {\"partition-id\": self.client_id}\n",
    "        \n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, rmse = test_regression(self.net, self.val_loader, self.device)\n",
    "        print(f\"[Client {self.client_id}] Evaluate -> Loss: {loss:.4f}, RMSE: {rmse:.4f}\")\n",
    "        return float(loss), len(self.val_loader.dataset), {\"RMSE\": float(rmse)}\n",
    "\n",
    "\n",
    "class DefaultStrategy(FedAvg):\n",
    "\n",
    "    # A custom strategy to store all the parameters.\n",
    "    # https://github.com/adap/flower/issues/487\n",
    "    # https://flower.ai/docs/framework/how-to-save-and-load-model-checkpoints.html\n",
    "\n",
    "    def __init__(self, model: type, total_round: int, only_last: bool = True, save_dir: str = \"models\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.model = model\n",
    "        self.total_round = total_round\n",
    "        self.only_last = only_last\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: list[tuple[ClientProxy, FitRes]],\n",
    "        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:\n",
    "        \"\"\"\n",
    "        Aggregate model weights using weighted average.\n",
    "        Also save each client's model and the global server model.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.only_last and server_round < self.total_round:\n",
    "            return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "\n",
    "        # For each client which returned FitRes, save the client model\n",
    "        for (_, fit_res) in results:\n",
    "            id_ = fit_res.metrics[\"partition-id\"]\n",
    "\n",
    "            client_parameters: Optional[Parameters] = fit_res.parameters\n",
    "            if client_parameters is not None:\n",
    "                net = self.model()\n",
    "                print(f\"[Round {server_round}] Saving model for client {id_}...\")\n",
    "\n",
    "                # Convert `Parameters` to `list[np.ndarray]`\n",
    "                client_ndarrays : list[np.ndarray] = parameters_to_ndarrays(\n",
    "                    client_parameters\n",
    "                )\n",
    "\n",
    "                # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "                params_dict = zip(net.state_dict().keys(), client_ndarrays)\n",
    "                state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "                net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "                # Save the model to disk\n",
    "                torch.save(net.state_dict(), f\"{self.save_dir}/client-{server_round}-{id_}.pth\")\n",
    "\n",
    "        # If `aggregated_parameters` is not None, update the global net and save it\n",
    "        if aggregated_parameters is not None:\n",
    "            net = self.model()\n",
    "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "            # Convert `Parameters` to `list[np.ndarray]`\n",
    "            aggregated_ndarrays: list[np.ndarray] = parameters_to_ndarrays(\n",
    "                aggregated_parameters\n",
    "            )\n",
    "\n",
    "            # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "            params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Save the model to disk\n",
    "            torch.save(net.state_dict(), f\"{self.save_dir}/server-{server_round}.pth\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following class to run the experiment\n",
    "\n",
    "# You need to provide the following information:\n",
    "# 1. The Network class (dont instantiate it)\n",
    "#       (assume we use the same network for all clients and server)\n",
    "# 2. The list of data loaders for each client,\n",
    "#       where loaders is a list of loader tuples (train, val, test)\n",
    "#       i.e. loaders = [ (train_loader_0, val_loader_0, test_loader_0), ... ]\n",
    "#       NOTE: In fit and evaluate, we ONLY use the train_loader and val_loader,\n",
    "#             But we ask you to pyt them together for simplicity for any future test use.\n",
    "#       NOTE: we assume the number of clients == number of data loaders\n",
    "# 3. Number of clients\n",
    "\n",
    "# See next block for an example of how to use this class\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "set_logger_propagation(logger, True)\n",
    "\n",
    "class FLExperiment:\n",
    "    \"\"\"\n",
    "    A federated learning experiment interface class.\n",
    "\n",
    "    NOTE: For each client, we now expect a tuple of three DataLoaders:\n",
    "    (train_loader, val_loader, test_loader).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_cls: type,\n",
    "        client_loaders: List[Tuple[DataLoader, DataLoader, DataLoader]],\n",
    "        num_clients: int,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        local_epochs: int = 1,\n",
    "        num_rounds: int = 5,\n",
    "        task_type: TaskType = TaskType.REGRESSION,\n",
    "        # strategy: Optional[Strategy] = None, # Is not supported yet. and may not be needed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_cls (type): A PyTorch nn.Module class (not an instance).\n",
    "                We'll instantiate `model_cls()` for each client and server.\n",
    "            client_loaders (List[(DataLoader, DataLoader, DataLoader)]):\n",
    "                A list of (train_loader, val_loader, test_loader) for each client.\n",
    "            num_clients (int): Number of clients to simulate.\n",
    "            device (torch.device): CPU or GPU device.\n",
    "            local_epochs (int): Local epochs on each client per round.\n",
    "            num_rounds (int): How many global training rounds.\n",
    "            strategy (Optional[Strategy]): Use a custom Flower strategy or fallback to default FedAvg.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing FLExperiment\")\n",
    "        if len(client_loaders) != num_clients:\n",
    "            raise ValueError(\n",
    "                f\"Number of client loader tuples ({len(client_loaders)}) does not match \"\n",
    "                f\"the number of clients ({num_clients}).\"\n",
    "            )\n",
    "\n",
    "        self.model_cls = model_cls\n",
    "        self.client_loaders = client_loaders\n",
    "        self.num_clients = num_clients\n",
    "        self.local_epochs = local_epochs\n",
    "        self.num_rounds = num_rounds\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "\n",
    "        # Store final trained models\n",
    "        self._client_models: List[Optional[nn.Module]] = [None] * self.num_clients\n",
    "        self._server_model: Optional[nn.Module] = None\n",
    "\n",
    "        # Create one model per client (instantiate model_cls)\n",
    "        self.client_nets = [self.model_cls().to(self.device) for _ in range(self.num_clients)]\n",
    "\n",
    "        self.strategy = self._create_default_strategy(save_only_last=True)\n",
    "        logger.info(\"FLExperiment initialized successfully\")\n",
    "        # # Use user-provided strategy or create a default one\n",
    "        # if strategy is None:\n",
    "        #     self.strategy = self._create_default_strategy()\n",
    "        # else:\n",
    "        #     self.strategy = strategy\n",
    "\n",
    "    def _create_default_strategy(self, save_only_last: bool) -> Strategy:\n",
    "        \"\"\"Create a default FedAvg strategy with a minimal server_evaluate.\"\"\"\n",
    "\n",
    "        logger.debug(\"Creating default strategy\")\n",
    "\n",
    "        def server_evaluate(\n",
    "            server_round: int,\n",
    "            parameters: NDArrays,\n",
    "            config: Dict[str, Scalar]\n",
    "        ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "            # Minimal server eval (no real evaluation)\n",
    "            net = self.model_cls().to(self.device)\n",
    "            set_parameters(net, parameters)\n",
    "            print(f\"[Server] Round {server_round} - no global evaluation implemented.\")\n",
    "            return None\n",
    "        \n",
    "        def weighted_average(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
    "            accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "            examples = [num_examples for num_examples, _ in metrics]\n",
    "            if sum(examples) == 0:\n",
    "                return {\"accuracy\": 0.0}\n",
    "            return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "        def weighted_average_regression(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
    "            total_sum_of_squares = 0.0\n",
    "            total_samples = 0\n",
    "            for (num_examples, m) in metrics:\n",
    "                #print(f'Test m contents: {m}')\n",
    "                if \"RMSE\" in m:\n",
    "                    total_sum_of_squares += m[\"RMSE\"]\n",
    "                    total_samples += num_examples\n",
    "            if total_samples == 0:\n",
    "                return {\"rmse\": 0.0}\n",
    "            rmse = (total_sum_of_squares / total_samples) ** 0.5\n",
    "            return {\"rmse\": rmse}\n",
    "        \n",
    "        if self.task_type == TaskType.CLASSFICATION:\n",
    "            aggregation_fn = weighted_average\n",
    "        else:\n",
    "            aggregation_fn = weighted_average_regression\n",
    "\n",
    "        default_strategy = DefaultStrategy(\n",
    "            model = self.model_cls,\n",
    "            total_round = self.num_rounds,\n",
    "            only_last = True,\n",
    "            fraction_fit=1.0,\n",
    "            fraction_evaluate=1.0,\n",
    "            min_fit_clients=self.num_clients,\n",
    "            min_evaluate_clients=self.num_clients,\n",
    "            min_available_clients=self.num_clients,\n",
    "            evaluate_fn=server_evaluate,\n",
    "            evaluate_metrics_aggregation_fn=aggregation_fn,\n",
    "        )\n",
    "        return default_strategy\n",
    "\n",
    "    def _client_fn(self, context: Context) -> Client:\n",
    "        \"\"\"Construct one Flower client using the partition_id to pick (train, val, test).\"\"\"\n",
    "        partition_id = context.node_config[\"partition-id\"]\n",
    "        trainloader, valloader, testloader = self.client_loaders[partition_id]\n",
    "        net = self.client_nets[partition_id]\n",
    "        logger.info(f\"Creating client {partition_id}\")\n",
    "\n",
    "        client = HousePricingClient(\n",
    "            net=net,\n",
    "            train_loader=trainloader,\n",
    "            val_loader=valloader,\n",
    "            test_loader = testloader,\n",
    "            device=self.device,\n",
    "            client_id=partition_id,\n",
    "            epochs=self.local_epochs\n",
    "        )\n",
    "        return client.to_client()\n",
    "\n",
    "    def _server_fn(self, context: Context) -> ServerAppComponents:\n",
    "        \"\"\"Server-side: configure strategy and server config.\"\"\"\n",
    "        config = ServerConfig(num_rounds=self.num_rounds)\n",
    "        return ServerAppComponents(strategy=self.strategy, config=config)\n",
    "\n",
    "    def run(self, save_only_last: bool = True) -> None:\n",
    "        \"\"\"Run the federated learning simulation and store final client/server models.\n",
    "        \n",
    "        Args:\n",
    "            save_only_last (bool): Save only the last round of models.\n",
    "                Default True. If False, all models will be saved.\n",
    "        \"\"\"\n",
    "        print(\"[FLExperiment] Starting federated training...\")\n",
    "        self.strategy = self._create_default_strategy(save_only_last=save_only_last)\n",
    "        client_app = ClientApp(client_fn=self._client_fn)\n",
    "        server_app = ServerApp(server_fn=self._server_fn)\n",
    "\n",
    "        # Resource allocation\n",
    "        if self.device.type == \"cuda\":\n",
    "            backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "        else:\n",
    "            backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "        # Run the simulation\n",
    "        run_simulation(\n",
    "            client_app=client_app,\n",
    "            server_app=server_app,\n",
    "            num_supernodes=self.num_clients,\n",
    "            backend_config=backend_config,\n",
    "        )\n",
    "        print(\"[FLExperiment] Federated training finished.\")\n",
    "\n",
    "    def get_clients(self, round_num: int = 0) -> List[nn.Module]:\n",
    "        \"\"\"Return final trained models for all clients (if they have been saved).\n",
    "        \n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            List[nn.Module]: List of final trained models for all clients.\n",
    "                The index of the list corresponds to the client ID \n",
    "                and the index of dataloader. \n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            return [\n",
    "                torch.load(f\"models/client-{round_num}-{cid}.pth\", map_location=self.device, weights_only=True)\n",
    "                for cid in range(self.num_clients)\n",
    "            ]\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Client models are not available. Have you called run() or set only_last=True?\")\n",
    "    \n",
    "    def get_client_dataloader_tuples(self, round_num: int = 0) -> List[Tuple[nn.Module, Tuple[DataLoader, DataLoader, DataLoader]]]:\n",
    "        \"\"\"Return the dataloaders for all clients.\n",
    "         \n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[nn.Module, Tuple[DataLoader, DataLoader, DataLoader]]]:\n",
    "                List of (client_model, (train_loader, val_loader, test_loader))\n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            clients = self.get_clients(round_num)\n",
    "            return list(zip(clients, self.client_loaders)) \n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Client dataloaders are not available. Have you called run() or set only_last=True?\")\n",
    "\n",
    "    def get_server(self, round_num: int = 0) -> nn.Module:\n",
    "        \"\"\"Return the final server model (if stored).\n",
    "\n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The final server model.\n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            return torch.load(f\"models/server-{round_num}.pth\", map_location=self.device, weights_only=True)\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Server model is not available. Have you called run() or set only_last=True?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Neural Network for Tabular Data\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size = 8):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class EZNet(nn.Module):\n",
    "    def __init__(self, in_features=8):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, in_features]\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders for House Pricing Dataset\n",
    "\n",
    "def load_data(client_id, seed=42):\n",
    "    data_path = f\"house_pricing_datasets_0_rouge/client_{client_id}.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Encode categorical features\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    # Separate features and target\n",
    "    features = df.drop(columns=[\"House Price\"]).values\n",
    "    target = df[\"House Price\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "    # Create a full dataset\n",
    "    full_dataset = TensorDataset(features_tensor, target_tensor)\n",
    "\n",
    "    # Determine lengths for splits\n",
    "    total_len = len(full_dataset)\n",
    "    train_len = int(0.7 * total_len)\n",
    "    val_len = int(0.2 * total_len)\n",
    "    test_len = total_len - train_len - val_len\n",
    "\n",
    "    # Use random_split for reproducible splits\n",
    "    generator = torch.Generator().manual_seed(seed + client_id)  # Client-specific seed\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset,\n",
    "        lengths=[train_len, val_len, test_len],\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    '''# Split into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(features))\n",
    "    val_size = int(0.2 * len(features))\n",
    "    test_size = len(features) - train_size - val_size\n",
    "    indices = np.random.permutation(len(features))\n",
    "    train_idx, val_idx, test_idx = indices[:train_size], indices[train_size:train_size + val_size], indices[train_size + val_size:]\n",
    "\n",
    "    X_train, y_train = features[train_idx], target[train_idx]\n",
    "    X_val, y_val = features[val_idx], target[val_idx]\n",
    "    X_test, y_test = features[test_idx], target[test_idx]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)'''\n",
    "\n",
    "    print(f\"Client {client_id} stats: mean={df['House Price'].mean()}, std={df['House Price'].std()}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 stats: mean=2655.6000458962008, std=996.1192688513912\n",
      "Client 2 stats: mean=2574.611936598437, std=1004.0983782459657\n",
      "Client 3 stats: mean=2679.56855169879, std=1063.9333406747944\n",
      "Client 4 stats: mean=2576.9200760471967, std=1064.3892717656709\n",
      "Client 5 stats: mean=3010.5561693498457, std=986.2380537998968\n"
     ]
    }
   ],
   "source": [
    "# Define loaders for all clients\n",
    "NUM_CLIENTS = 5\n",
    "loaders = [\n",
    "    load_data(client_id) for client_id in range(1, NUM_CLIENTS + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 17:28:03,364 - INFO - Initializing FLExperiment\n",
      "2025-01-09 17:28:03,375 - DEBUG - Creating default strategy\n",
      "2025-01-09 17:28:03,389 - INFO - FLExperiment initialized successfully\n",
      "2025-01-09 17:28:03,392 - DEBUG - Creating default strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FLExperiment] Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 0 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 1 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=43892)\u001b[0m [Client 3] Evaluate -> Loss: 8128071.0857, RMSE: 2850.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 2 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 3 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 4 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 5 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=43892)\u001b[0m [Client 3] Evaluate -> Loss: 8128071.0857, RMSE: 2850.9772\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 6 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 7 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 8 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 9 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=42384)\u001b[0m [Client 4] Evaluate -> Loss: 11612237.8667, RMSE: 3407.6734\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 10 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 11]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 11 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 12]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 12 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 13]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 13 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 14]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=43892)\u001b[0m [Client 0] Evaluate -> Loss: 5877642.0000, RMSE: 2424.3849\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 14 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 15]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 15 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 16]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 16 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 17]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 17 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 18]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=43892)\u001b[0m [Client 2] Evaluate -> Loss: 5562179.6923, RMSE: 2358.4274\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 18 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 19]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 19 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 20] Saving model for client 1...\n",
      "[Round 20] Saving model for client 3...\n",
      "[Round 20] Saving model for client 2...\n",
      "[Round 20] Saving model for client 0...\n",
      "[Round 20] Saving model for client 4...\n",
      "Saving round 20 aggregated_parameters...\n",
      "[Server] Round 20 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 20 round(s) in 54.47s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 6: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 7: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 8: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 9: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 10: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 11: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 12: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 13: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 14: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 15: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 16: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 17: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 18: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 19: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 20: 7335468.571428572\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'rmse': [(1, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (2, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (3, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (4, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (5, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (6, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (7, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (8, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (9, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (10, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (11, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (12, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (13, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (14, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (15, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (16, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (17, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (18, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (19, 10.669588946513581),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (20, 10.669588946513581)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=42880)\u001b[0m [Client 1] Evaluate -> Loss: 6277592.5000, RMSE: 2505.5124\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "[FLExperiment] Federated training finished.\n"
     ]
    }
   ],
   "source": [
    "fl_exp = FLExperiment(\n",
    "    model_cls=EZNet,\n",
    "    client_loaders=loaders,\n",
    "    num_clients=5,\n",
    "    num_rounds=20,\n",
    "    local_epochs=40,\n",
    "    task_type = TaskType.REGRESSION\n",
    ")\n",
    "\n",
    "\n",
    "fl_exp.run(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 rmse: 2721.624882308361\n",
      "Client 0 rmse: 2721.624882308361\n"
     ]
    }
   ],
   "source": [
    "# Test the client model 0 on the corresponding test set\n",
    "\n",
    "clients = fl_exp.get_clients()\n",
    "model0_1 = Net().to(DEVICE)\n",
    "model0_1.load_state_dict(clients[0])\n",
    "model0_1.eval()\n",
    "\n",
    "_, _, test_loader_1 = loaders[0]\n",
    "loss, rmse = test_regression(model0_1, test_loader_1)\n",
    "print(f\"Client 0 rmse: {rmse}\")\n",
    "\n",
    "# Or Alternatively\n",
    "client_dls = fl_exp.get_client_dataloader_tuples()\n",
    "model0_2 = Net().to(DEVICE)\n",
    "model0_2.load_state_dict(client_dls[0][0])\n",
    "model0_2.eval()\n",
    "\n",
    "_, _, test_loader_2 = client_dls[0][1]\n",
    "loss, rmse = test_regression(model0_2, test_loader_2)\n",
    "print(f\"Client 0 rmse: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client_0 loss: 10418487.384615384, RMSE: 3227.7681739268983\n",
      "Client_1 loss: 8274908.235294118, RMSE: 2876.614022647828\n",
      "Client_2 loss: 5841198.285714285, RMSE: 2416.857109080776\n",
      "Client_3 loss: 5873791.555555556, RMSE: 2423.590632832937\n",
      "Client_4 loss: 6741237.777777778, RMSE: 2596.389373298577\n"
     ]
    }
   ],
   "source": [
    "clients = fl_exp.get_clients()\n",
    "for i, client in enumerate(clients):\n",
    "    model = Net().to(DEVICE)\n",
    "    model.load_state_dict(client)\n",
    "    model.eval()\n",
    "\n",
    "    _, _, test_loader = loaders[i]\n",
    "    loss, rmse = test_regression(model, test_loader)\n",
    "    print(f'Client_{i} loss: {loss}, RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client_0 loss: 6709510.153846154, RMSE: 2590.2722161668944\n",
      "Client_1 loss: 5882296.94117647, RMSE: 2425.3447056401014\n",
      "Client_2 loss: 6273306.285714285, RMSE: 2504.656919762522\n",
      "Client_3 loss: 8848950.222222222, RMSE: 2974.7185114262866\n",
      "Client_4 loss: 7861413.333333333, RMSE: 2803.821202097832\n"
     ]
    }
   ],
   "source": [
    "clients = fl_exp.get_clients()\n",
    "for i, client in enumerate(clients):\n",
    "    model = EZNet().to(DEVICE)\n",
    "    model.load_state_dict(client)\n",
    "    model.eval()\n",
    "\n",
    "    _, _, test_loader = loaders[i]\n",
    "    loss, rmse = test_regression(model, test_loader)\n",
    "    print(f'Client_{i} loss: {loss}, RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 302928038.6976744\n",
      "Epoch 2: train loss 305517008.372093\n",
      "Epoch 3: train loss 304177497.3023256\n",
      "Epoch 4: train loss 302526589.0232558\n",
      "Epoch 5: train loss 294715213.39534885\n",
      "Epoch 6: train loss 298931036.2790698\n",
      "Epoch 7: train loss 288432360.18604654\n",
      "Epoch 8: train loss 292556859.53488374\n",
      "Epoch 9: train loss 293066195.3488372\n",
      "Epoch 10: train loss 287413200.372093\n",
      "Epoch 11: train loss 288174312.18604654\n",
      "Epoch 12: train loss 286486980.46511626\n",
      "Epoch 13: train loss 285489866.4186047\n",
      "Epoch 14: train loss 275262684.2790698\n",
      "Epoch 15: train loss 277892608.0\n",
      "Epoch 16: train loss 281704296.18604654\n",
      "Epoch 17: train loss 277394807.0697674\n",
      "Epoch 18: train loss 269158060.6511628\n",
      "Epoch 19: train loss 265689790.5116279\n",
      "Epoch 20: train loss 269790848.0\n",
      "Epoch 21: train loss 266794287.62790698\n",
      "Epoch 22: train loss 264259595.90697673\n",
      "Epoch 23: train loss 262091609.30232558\n",
      "Epoch 24: train loss 260540026.04651162\n",
      "Epoch 25: train loss 254112077.39534885\n",
      "Epoch 26: train loss 251303566.88372093\n",
      "Epoch 27: train loss 254056061.02325583\n",
      "Epoch 28: train loss 253052332.6511628\n",
      "Epoch 29: train loss 250107073.4883721\n",
      "Epoch 30: train loss 250936903.44186047\n",
      "Epoch 31: train loss 250491984.37209302\n",
      "Epoch 32: train loss 248559565.39534885\n",
      "Epoch 33: train loss 247631419.5348837\n",
      "Epoch 34: train loss 245239590.69767442\n",
      "Epoch 35: train loss 236953945.30232558\n",
      "Epoch 36: train loss 236635005.02325583\n",
      "Epoch 37: train loss 232926160.37209302\n",
      "Epoch 38: train loss 232923862.3255814\n",
      "Epoch 39: train loss 230723113.6744186\n",
      "Epoch 40: train loss 234369705.6744186\n",
      "Epoch 41: train loss 233204116.8372093\n",
      "Epoch 42: train loss 224487572.8372093\n",
      "Epoch 43: train loss 226335172.4651163\n",
      "Epoch 44: train loss 219964987.5348837\n",
      "Epoch 45: train loss 222139082.41860464\n",
      "Epoch 46: train loss 219917136.37209302\n",
      "Epoch 47: train loss 219343360.0\n",
      "Epoch 48: train loss 218395106.23255813\n",
      "Epoch 49: train loss 215383587.72093022\n",
      "Epoch 50: train loss 214832928.74418604\n",
      "Epoch 51: train loss 212808522.41860464\n",
      "Epoch 52: train loss 206986865.11627907\n",
      "Epoch 53: train loss 214676450.23255813\n",
      "Epoch 54: train loss 209534898.60465115\n",
      "Epoch 55: train loss 205483865.30232558\n",
      "Epoch 56: train loss 203505146.04651162\n",
      "Epoch 57: train loss 204098667.1627907\n",
      "Epoch 58: train loss 206328236.6511628\n",
      "Epoch 59: train loss 202294269.02325583\n",
      "Epoch 60: train loss 201741835.90697673\n",
      "Epoch 61: train loss 201162656.74418604\n",
      "Epoch 62: train loss 198122829.39534885\n",
      "Epoch 63: train loss 192461273.30232558\n",
      "Epoch 64: train loss 191260951.8139535\n",
      "Epoch 65: train loss 196616248.55813953\n",
      "Epoch 66: train loss 193437279.25581396\n",
      "Epoch 67: train loss 193007357.02325583\n",
      "Epoch 68: train loss 189345640.1860465\n",
      "Epoch 69: train loss 184524713.6744186\n",
      "Epoch 70: train loss 189459610.79069766\n",
      "Epoch 71: train loss 184614798.88372093\n",
      "Epoch 72: train loss 186245935.62790698\n",
      "Epoch 73: train loss 178864443.5348837\n",
      "Epoch 74: train loss 179210451.3488372\n",
      "Epoch 75: train loss 177938408.1860465\n",
      "Epoch 76: train loss 180552439.06976745\n",
      "Epoch 77: train loss 179095424.0\n",
      "Epoch 78: train loss 171492334.1395349\n",
      "Epoch 79: train loss 175494400.0\n",
      "Epoch 80: train loss 171157492.09302327\n",
      "Epoch 81: train loss 173277172.09302327\n",
      "Epoch 82: train loss 169321159.44186047\n",
      "Epoch 83: train loss 169875845.95348838\n",
      "Epoch 84: train loss 169778199.8139535\n",
      "Epoch 85: train loss 165413885.02325583\n",
      "Epoch 86: train loss 170543947.90697673\n",
      "Epoch 87: train loss 162042278.69767442\n",
      "Epoch 88: train loss 165060905.6744186\n",
      "Epoch 89: train loss 160027198.5116279\n",
      "Epoch 90: train loss 161166538.41860464\n",
      "Epoch 91: train loss 161042527.25581396\n",
      "Epoch 92: train loss 161877368.55813953\n",
      "Epoch 93: train loss 159988463.62790698\n",
      "Epoch 94: train loss 158820564.8372093\n",
      "Epoch 95: train loss 154833223.44186047\n",
      "Epoch 96: train loss 154136775.44186047\n",
      "Epoch 97: train loss 153577680.37209302\n",
      "Epoch 98: train loss 146204264.1860465\n",
      "Epoch 99: train loss 152764285.02325583\n",
      "Epoch 100: train loss 149691005.02325583\n",
      "(3342309.3333333335, 1828.198384566985)\n"
     ]
    }
   ],
   "source": [
    "eznet = EZNet()\n",
    "train(eznet, loaders[0][0], epochs=100, verbose=True, task_type=TaskType.REGRESSION)\n",
    "print(test_regression(eznet, loaders[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047]) tensor(0.0083)\n",
      "tensor([ 0.2779,  0.7342, -0.3736, -0.3952, -1.2449, -0.4260, -0.9261,  0.3349]) tensor(-3.0282)\n",
      "tensor([-0.9757, -1.3057,  1.8161,  1.0204, -0.5522,  0.5234,  0.4719, -0.2912]) tensor(1.0606)\n",
      "tensor([-0.6554, -0.1144, -1.0761, -0.3204,  0.0177, -0.8664, -0.1684, -1.2356]) tensor(-6.6284)\n",
      "Epoch 1: train loss 16.926950923374722\n",
      "Epoch 2: train loss 16.10869864327567\n",
      "Epoch 3: train loss 15.310064054216657\n",
      "Epoch 4: train loss 14.559073726109096\n",
      "Epoch 5: train loss 13.840103334699359\n",
      "Epoch 6: train loss 13.141866607666016\n",
      "Epoch 7: train loss 12.47857789175851\n",
      "Epoch 8: train loss 11.84029914855957\n",
      "Epoch 9: train loss 11.2280201285226\n",
      "Epoch 10: train loss 10.639702268327985\n",
      "Epoch 11: train loss 10.07326446533203\n",
      "Epoch 12: train loss 9.534879259381976\n",
      "Epoch 13: train loss 9.01893990107945\n",
      "Epoch 14: train loss 8.519811940874373\n",
      "Epoch 15: train loss 8.043475701468331\n",
      "Epoch 16: train loss 7.58717914036342\n",
      "Epoch 17: train loss 7.150384973798479\n",
      "Epoch 18: train loss 6.731913615635463\n",
      "Epoch 19: train loss 6.331510265895298\n",
      "Epoch 20: train loss 5.953755836486817\n",
      "(4.934424524307251, 2.2213564604329603)\n"
     ]
    }
   ],
   "source": [
    "def generate_random_dataset(n_samples=10000, n_features=5):\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    # Y = torch.zeros(n_samples) \n",
    "    Y = 1.5 * X.sum(dim=1)\n",
    "    print(X[0], Y[0])\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_random_client_loaders(\n",
    "    num_clients=4,\n",
    "    n_samples_per_client=1000,\n",
    "    n_features=8,\n",
    "    batch_size=16,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    seed=42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    client_loaders = []\n",
    "    for client_id in range(num_clients):\n",
    "        dataset = generate_random_dataset(n_samples_per_client, n_features)\n",
    "        total_len = len(dataset)\n",
    "        train_len = int(train_ratio * total_len)\n",
    "        val_len = int(val_ratio * total_len)\n",
    "        test_len = total_len - train_len - val_len\n",
    "\n",
    "        train_ds, val_ds, test_ds = random_split(\n",
    "            dataset,\n",
    "            lengths=[train_len, val_len, test_len],\n",
    "            generator=torch.Generator().manual_seed(seed + client_id)\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "        client_loaders.append((train_loader, val_loader, test_loader))\n",
    "\n",
    "    return client_loaders\n",
    "\n",
    "loaders_regression = create_random_client_loaders(num_clients=4)\n",
    "\n",
    "eznet = EZNet()\n",
    "train(eznet, loaders_regression[0][0], epochs=20, verbose=True, task_type=TaskType.REGRESSION)\n",
    "print(test_regression(eznet, loaders_regression[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
