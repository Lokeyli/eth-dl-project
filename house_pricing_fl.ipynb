{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barbi\\Documents\\ETH\\HS 2024\\Deep Learning\\eth-dl-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-09 14:03:56,354\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.14.0 / PyTorch 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys,os,os.path\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents, ClientManager\n",
    "from flwr.server.strategy import Strategy, FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context\n",
    "from flwr.common import FitRes, Parameters, parameters_to_ndarrays\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common.logger import set_logger_propagation\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device(device)  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {fl.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the above ugly code of flower client into a class\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "class TaskType(Enum):\n",
    "\n",
    "    CLASSFICATION = 0\n",
    "    REGRESSION = 1\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def train(net, trainloader, epochs: int, verbose=False, device = \"cpu\", task_type = TaskType.CLASSFICATION):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    if task_type == TaskType.CLASSFICATION:    \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    elif task_type == TaskType.REGRESSION:\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            if task_type == TaskType.REGRESSION:\n",
    "                outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            if task_type == TaskType.CLASSFICATION:\n",
    "                correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        if verbose:\n",
    "            if task_type == TaskType.CLASSFICATION:\n",
    "                epoch_acc = correct / total\n",
    "                print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "            elif task_type == TaskType.REGRESSION:\n",
    "                print(f\"Epoch {epoch+1}: train loss {epoch_loss}\")\n",
    "\n",
    "def test(net, testloader, device = \"cpu\"):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "def test_regression(net, testloader, device=\"cpu\"):\n",
    "    \"\"\"Evaluate the regression model on the entire test set.\"\"\"\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "    sum_of_squares, total_samples = 0.0, 0\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in testloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            target = y.view(-1)\n",
    "            outputs = net(x).view(-1) # NOTE: You maight want to modify this part\n",
    "            sum_of_squares += criterion(outputs, target).item()\n",
    "            total_samples += len(y)\n",
    "\n",
    "    if total_samples > 0:\n",
    "        avg_mse = sum_of_squares / total_samples\n",
    "    else:\n",
    "        avg_mse = 0.0\n",
    "    avg_loss = avg_mse\n",
    "\n",
    "    # Note that to make sure the consistence,\n",
    "    # we return mse twice to match {loss, accurancy as the test function}\n",
    "    return avg_loss, avg_mse ** 0.5\n",
    "\n",
    "# Custom Client Class\n",
    "class FLClient(NumPyClient):\n",
    "    \"\"\"A Flower client that holds its own model and training data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        device: torch.device,\n",
    "        client_id: int,\n",
    "        epochs: int = 1,\n",
    "        task_type: TaskType = TaskType.CLASSFICATION\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.epochs = epochs\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> List[np.ndarray]:\n",
    "        \"\"\"Return the current local model parameters.\"\"\"\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, device=self.device, epochs=self.epochs, verbose=False, task_type = self.task_type)\n",
    "        new_params = get_parameters(self.net)\n",
    "        # Return partition-id in the metrics\n",
    "        # The simplest way to store the model\n",
    "        return new_params, len(self.trainloader.dataset), {\"partition-id\": self.client_id}\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        if self.task_type == TaskType.CLASSFICATION:\n",
    "            loss, accuracy = test(self.net, self.valloader, self.device)\n",
    "            print(f\"[Client {self.client_id}] Evaluate -> Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            return float(loss), len(self.valloader.dataset), {\"accuracy\": float(accuracy)}\n",
    "        elif self.task_type == TaskType.REGRESSION:\n",
    "            loss, mse = test_regression(self.net, self.valloader, self.device)\n",
    "            print(f\"[Client {self.client_id}] Evaluate -> Loss: {mse:.4f}\")\n",
    "            return float(loss), len(self.valloader.dataset), {\"MSE\": float(mse)}\n",
    "\n",
    "# Custom Client for House Pricing Dataset\n",
    "\n",
    "class HousePricingClient(fl.client.NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        device: torch.device,\n",
    "        client_id: int,\n",
    "        epochs: int = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> List[np.ndarray]:\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    '''def set_parameters(self, parameters):\n",
    "        state_dict = dict(zip(self.model.state_dict().keys(), parameters))\n",
    "        self.model.load_state_dict({k: torch.tensor(v) for k, v in state_dict.items()})'''\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[List[np.ndarray], int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.train_loader, device=self.device, epochs=self.epochs, verbose=False)\n",
    "        new_params = get_parameters(self.net)\n",
    "        # Return partition-id in the metrics\n",
    "        # The simplest way to store the model\n",
    "        return new_params, len(self.train_loader.dataset), {\"partition-id\": self.client_id}\n",
    "        \n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List[np.ndarray], config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, Dict[str, Scalar]]:\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, rmse = test_regression(self.net, self.val_loader, self.device)\n",
    "        print(f\"[Client {self.client_id}] Evaluate -> Loss: {loss:.4f}, RMSE: {rmse:.4f}\")\n",
    "        return float(loss), len(self.val_loader.dataset), {\"RMSE\": float(rmse)}\n",
    "\n",
    "\n",
    "class DefaultStrategy(FedAvg):\n",
    "\n",
    "    # A custom strategy to store all the parameters.\n",
    "    # https://github.com/adap/flower/issues/487\n",
    "    # https://flower.ai/docs/framework/how-to-save-and-load-model-checkpoints.html\n",
    "\n",
    "    def __init__(self, model: type, total_round: int, only_last: bool = True, save_dir: str = \"models\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.model = model\n",
    "        self.total_round = total_round\n",
    "        self.only_last = only_last\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: list[tuple[ClientProxy, FitRes]],\n",
    "        failures: list[Union[tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> tuple[Optional[Parameters], dict[str, Scalar]]:\n",
    "        \"\"\"\n",
    "        Aggregate model weights using weighted average.\n",
    "        Also save each client's model and the global server model.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.only_last and server_round < self.total_round:\n",
    "            return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "\n",
    "        # For each client which returned FitRes, save the client model\n",
    "        for (_, fit_res) in results:\n",
    "            id_ = fit_res.metrics[\"partition-id\"]\n",
    "\n",
    "            client_parameters: Optional[Parameters] = fit_res.parameters\n",
    "            if client_parameters is not None:\n",
    "                net = self.model()\n",
    "                print(f\"[Round {server_round}] Saving model for client {id_}...\")\n",
    "\n",
    "                # Convert `Parameters` to `list[np.ndarray]`\n",
    "                client_ndarrays : list[np.ndarray] = parameters_to_ndarrays(\n",
    "                    client_parameters\n",
    "                )\n",
    "\n",
    "                # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "                params_dict = zip(net.state_dict().keys(), client_ndarrays)\n",
    "                state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "                net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "                # Save the model to disk\n",
    "                torch.save(net.state_dict(), f\"{self.save_dir}/client-{server_round}-{id_}.pth\")\n",
    "\n",
    "        # If `aggregated_parameters` is not None, update the global net and save it\n",
    "        if aggregated_parameters is not None:\n",
    "            net = self.model()\n",
    "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "            # Convert `Parameters` to `list[np.ndarray]`\n",
    "            aggregated_ndarrays: list[np.ndarray] = parameters_to_ndarrays(\n",
    "                aggregated_parameters\n",
    "            )\n",
    "\n",
    "            # Convert `list[np.ndarray]` to PyTorch `state_dict`\n",
    "            params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Save the model to disk\n",
    "            torch.save(net.state_dict(), f\"{self.save_dir}/server-{server_round}.pth\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following class to run the experiment\n",
    "\n",
    "# You need to provide the following information:\n",
    "# 1. The Network class (dont instantiate it)\n",
    "#       (assume we use the same network for all clients and server)\n",
    "# 2. The list of data loaders for each client,\n",
    "#       where loaders is a list of loader tuples (train, val, test)\n",
    "#       i.e. loaders = [ (train_loader_0, val_loader_0, test_loader_0), ... ]\n",
    "#       NOTE: In fit and evaluate, we ONLY use the train_loader and val_loader,\n",
    "#             But we ask you to pyt them together for simplicity for any future test use.\n",
    "#       NOTE: we assume the number of clients == number of data loaders\n",
    "# 3. Number of clients\n",
    "\n",
    "# See next block for an example of how to use this class\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "set_logger_propagation(logger, True)\n",
    "\n",
    "class FLExperiment:\n",
    "    \"\"\"\n",
    "    A federated learning experiment interface class.\n",
    "\n",
    "    NOTE: For each client, we now expect a tuple of three DataLoaders:\n",
    "    (train_loader, val_loader, test_loader).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_cls: type,\n",
    "        client_loaders: List[Tuple[DataLoader, DataLoader, DataLoader]],\n",
    "        num_clients: int,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        local_epochs: int = 1,\n",
    "        num_rounds: int = 5,\n",
    "        task_type: TaskType = TaskType.REGRESSION,\n",
    "        # strategy: Optional[Strategy] = None, # Is not supported yet. and may not be needed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_cls (type): A PyTorch nn.Module class (not an instance).\n",
    "                We'll instantiate `model_cls()` for each client and server.\n",
    "            client_loaders (List[(DataLoader, DataLoader, DataLoader)]):\n",
    "                A list of (train_loader, val_loader, test_loader) for each client.\n",
    "            num_clients (int): Number of clients to simulate.\n",
    "            device (torch.device): CPU or GPU device.\n",
    "            local_epochs (int): Local epochs on each client per round.\n",
    "            num_rounds (int): How many global training rounds.\n",
    "            strategy (Optional[Strategy]): Use a custom Flower strategy or fallback to default FedAvg.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing FLExperiment\")\n",
    "        if len(client_loaders) != num_clients:\n",
    "            raise ValueError(\n",
    "                f\"Number of client loader tuples ({len(client_loaders)}) does not match \"\n",
    "                f\"the number of clients ({num_clients}).\"\n",
    "            )\n",
    "\n",
    "        self.model_cls = model_cls\n",
    "        self.client_loaders = client_loaders\n",
    "        self.num_clients = num_clients\n",
    "        self.local_epochs = local_epochs\n",
    "        self.num_rounds = num_rounds\n",
    "        self.device = device\n",
    "        self.task_type = task_type\n",
    "\n",
    "        # Store final trained models\n",
    "        self._client_models: List[Optional[nn.Module]] = [None] * self.num_clients\n",
    "        self._server_model: Optional[nn.Module] = None\n",
    "\n",
    "        # Create one model per client (instantiate model_cls)\n",
    "        self.client_nets = [self.model_cls().to(self.device) for _ in range(self.num_clients)]\n",
    "\n",
    "        self.strategy = self._create_default_strategy(save_only_last=True)\n",
    "        logger.info(\"FLExperiment initialized successfully\")\n",
    "        # # Use user-provided strategy or create a default one\n",
    "        # if strategy is None:\n",
    "        #     self.strategy = self._create_default_strategy()\n",
    "        # else:\n",
    "        #     self.strategy = strategy\n",
    "\n",
    "    def _create_default_strategy(self, save_only_last: bool) -> Strategy:\n",
    "        \"\"\"Create a default FedAvg strategy with a minimal server_evaluate.\"\"\"\n",
    "\n",
    "        logger.debug(\"Creating default strategy\")\n",
    "\n",
    "        def server_evaluate(\n",
    "            server_round: int,\n",
    "            parameters: NDArrays,\n",
    "            config: Dict[str, Scalar]\n",
    "        ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "            # Minimal server eval (no real evaluation)\n",
    "            net = self.model_cls().to(self.device)\n",
    "            set_parameters(net, parameters)\n",
    "            print(f\"[Server] Round {server_round} - no global evaluation implemented.\")\n",
    "            return None\n",
    "        \n",
    "        def weighted_average(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
    "            accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "            examples = [num_examples for num_examples, _ in metrics]\n",
    "            if sum(examples) == 0:\n",
    "                return {\"accuracy\": 0.0}\n",
    "            return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "        def weighted_average_regression(metrics: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
    "            total_sum_of_squares = 0.0\n",
    "            total_samples = 0\n",
    "            for (num_examples, m) in metrics:\n",
    "                #print(f'Test m contents: {m}')\n",
    "                if \"RMSE\" in m:\n",
    "                    total_sum_of_squares += m[\"RMSE\"]\n",
    "                    total_samples += num_examples\n",
    "            if total_samples == 0:\n",
    "                return {\"rmse\": 0.0}\n",
    "            rmse = (total_sum_of_squares / total_samples) ** 0.5\n",
    "            return {\"rmse\": rmse}\n",
    "        \n",
    "        if self.task_type == TaskType.CLASSFICATION:\n",
    "            aggregation_fn = weighted_average\n",
    "        else:\n",
    "            aggregation_fn = weighted_average_regression\n",
    "\n",
    "        default_strategy = DefaultStrategy(\n",
    "            model = self.model_cls,\n",
    "            total_round = self.num_rounds,\n",
    "            only_last = True,\n",
    "            fraction_fit=1.0,\n",
    "            fraction_evaluate=1.0,\n",
    "            min_fit_clients=self.num_clients,\n",
    "            min_evaluate_clients=self.num_clients,\n",
    "            min_available_clients=self.num_clients,\n",
    "            evaluate_fn=server_evaluate,\n",
    "            evaluate_metrics_aggregation_fn=aggregation_fn,\n",
    "        )\n",
    "        return default_strategy\n",
    "\n",
    "    def _client_fn(self, context: Context) -> Client:\n",
    "        \"\"\"Construct one Flower client using the partition_id to pick (train, val, test).\"\"\"\n",
    "        partition_id = context.node_config[\"partition-id\"]\n",
    "        trainloader, valloader, testloader = self.client_loaders[partition_id]\n",
    "        net = self.client_nets[partition_id]\n",
    "        logger.info(f\"Creating client {partition_id}\")\n",
    "\n",
    "        client = HousePricingClient(\n",
    "            net=net,\n",
    "            train_loader=trainloader,\n",
    "            val_loader=valloader,\n",
    "            test_loader = testloader,\n",
    "            device=self.device,\n",
    "            client_id=partition_id,\n",
    "            epochs=self.local_epochs\n",
    "        )\n",
    "        return client.to_client()\n",
    "\n",
    "    def _server_fn(self, context: Context) -> ServerAppComponents:\n",
    "        \"\"\"Server-side: configure strategy and server config.\"\"\"\n",
    "        config = ServerConfig(num_rounds=self.num_rounds)\n",
    "        return ServerAppComponents(strategy=self.strategy, config=config)\n",
    "\n",
    "    def run(self, save_only_last: bool = True) -> None:\n",
    "        \"\"\"Run the federated learning simulation and store final client/server models.\n",
    "        \n",
    "        Args:\n",
    "            save_only_last (bool): Save only the last round of models.\n",
    "                Default True. If False, all models will be saved.\n",
    "        \"\"\"\n",
    "        print(\"[FLExperiment] Starting federated training...\")\n",
    "        self.strategy = self._create_default_strategy(save_only_last=save_only_last)\n",
    "        client_app = ClientApp(client_fn=self._client_fn)\n",
    "        server_app = ServerApp(server_fn=self._server_fn)\n",
    "\n",
    "        # Resource allocation\n",
    "        if self.device.type == \"cuda\":\n",
    "            backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "        else:\n",
    "            backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "        # Run the simulation\n",
    "        run_simulation(\n",
    "            client_app=client_app,\n",
    "            server_app=server_app,\n",
    "            num_supernodes=self.num_clients,\n",
    "            backend_config=backend_config,\n",
    "        )\n",
    "        print(\"[FLExperiment] Federated training finished.\")\n",
    "\n",
    "    def get_clients(self, round_num: int = 0) -> List[nn.Module]:\n",
    "        \"\"\"Return final trained models for all clients (if they have been saved).\n",
    "        \n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            List[nn.Module]: List of final trained models for all clients.\n",
    "                The index of the list corresponds to the client ID \n",
    "                and the index of dataloader. \n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            return [\n",
    "                torch.load(f\"models/client-{round_num}-{cid}.pth\", map_location=self.device, weights_only=True)\n",
    "                for cid in range(self.num_clients)\n",
    "            ]\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Client models are not available. Have you called run() or set only_last=True?\")\n",
    "    \n",
    "    def get_client_dataloader_tuples(self, round_num: int = 0) -> List[Tuple[nn.Module, Tuple[DataLoader, DataLoader, DataLoader]]]:\n",
    "        \"\"\"Return the dataloaders for all clients.\n",
    "         \n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[nn.Module, Tuple[DataLoader, DataLoader, DataLoader]]]:\n",
    "                List of (client_model, (train_loader, val_loader, test_loader))\n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            clients = self.get_clients(round_num)\n",
    "            return list(zip(clients, self.client_loaders)) \n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Client dataloaders are not available. Have you called run() or set only_last=True?\")\n",
    "\n",
    "    def get_server(self, round_num: int = 0) -> nn.Module:\n",
    "        \"\"\"Return the final server model (if stored).\n",
    "\n",
    "        Args:\n",
    "            round_num (int): Round number to fetch models from. Default 0 (last round).\n",
    "        \n",
    "        Returns:\n",
    "            nn.Module: The final server model.\n",
    "        \"\"\"\n",
    "        assert round_num <= self.num_rounds, f\"Round {round_num} not available, only {self.num_rounds} rounds.\"\n",
    "        if round_num <= 0:\n",
    "            round_num = self.num_rounds\n",
    "        try:\n",
    "            return torch.load(f\"models/server-{round_num}.pth\", map_location=self.device, weights_only=True)\n",
    "        except FileNotFoundError:\n",
    "            raise RuntimeError(\"Server model is not available. Have you called run() or set only_last=True?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Neural Network for Tabular Data\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size = 8):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class EZNet(nn.Module):\n",
    "    def __init__(self, in_features=8):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, in_features]\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders for House Pricing Dataset\n",
    "\n",
    "def load_data(client_id):\n",
    "    data_path = f\"house_pricing_datasets_0_rouge/client_{client_id}.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Encode categorical features\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = pd.Categorical(df[col]).codes\n",
    "\n",
    "    # Separate features and target\n",
    "    features = df.drop(columns=[\"House Price\"]).values\n",
    "    target = df[\"House Price\"].values.reshape(-1, 1)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(features))\n",
    "    val_size = int(0.2 * len(features))\n",
    "    test_size = len(features) - train_size - val_size\n",
    "    indices = np.random.permutation(len(features))\n",
    "    train_idx, val_idx, test_idx = indices[:train_size], indices[train_size:train_size + val_size], indices[train_size + val_size:]\n",
    "\n",
    "    X_train, y_train = features[train_idx], target[train_idx]\n",
    "    X_val, y_val = features[val_idx], target[val_idx]\n",
    "    X_test, y_test = features[test_idx], target[test_idx]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "\n",
    "    print(f\"Client {client_id} stats: mean={df['House Price'].mean()}, std={df['House Price'].std()}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 stats: mean=2655.6000458962008, std=996.1192688513912\n",
      "Client 2 stats: mean=2574.611936598437, std=1004.0983782459657\n",
      "Client 3 stats: mean=2679.56855169879, std=1063.9333406747944\n",
      "Client 4 stats: mean=2576.9200760471967, std=1064.3892717656709\n",
      "Client 5 stats: mean=3010.5561693498457, std=986.2380537998968\n"
     ]
    }
   ],
   "source": [
    "# Define loaders for all clients\n",
    "NUM_CLIENTS = 5\n",
    "loaders = [\n",
    "    load_data(client_id) for client_id in range(1, NUM_CLIENTS + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 17:14:47,520 - INFO - Initializing FLExperiment\n",
      "2025-01-09 17:14:47,524 - DEBUG - Creating default strategy\n",
      "2025-01-09 17:14:47,529 - INFO - FLExperiment initialized successfully\n",
      "2025-01-09 17:14:47,530 - DEBUG - Creating default strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=20, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FLExperiment] Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 0 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 1 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=4908)\u001b[0m [Client 0] Evaluate -> Loss: 7349988.6667, RMSE: 2711.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 2 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 3 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 4 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 5 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 6 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=4908)\u001b[0m [Client 4] Evaluate -> Loss: 11850235.7333, RMSE: 3442.4171\u001b[32m [repeated 25x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 7 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 8 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 9 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 10 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 11]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 11 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 12]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=4908)\u001b[0m [Client 1] Evaluate -> Loss: 7011443.5000, RMSE: 2647.9130\u001b[32m [repeated 26x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 12 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 13]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 13 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 14]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 14 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 15]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 15 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 16]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 16 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 17]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 17 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 18]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=4908)\u001b[0m [Client 4] Evaluate -> Loss: 11850234.6667, RMSE: 3442.4170\u001b[32m [repeated 29x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 18 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 19]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Round 19 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 20] Saving model for client 4...\n",
      "[Round 20] Saving model for client 0...\n",
      "[Round 20] Saving model for client 3...\n",
      "[Round 20] Saving model for client 1...\n",
      "[Round 20] Saving model for client 2...\n",
      "Saving round 20 aggregated_parameters...\n",
      "[Server] Round 20 - no global evaluation implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 20 round(s) in 28.09s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 6: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 7: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 8: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 9: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 10: 7499523.865546218\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 11: 7499523.6302521005\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 12: 7499523.6302521005\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 13: 7499523.6302521005\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 14: 7499523.6302521005\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 15: 7499523.6302521005\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 16: 7499523.49579832\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 17: 7499523.49579832\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 18: 7499522.8907563025\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 19: 7499522.8907563025\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 20: 7499522.957983193\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'rmse': [(1, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (2, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (3, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (4, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (5, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (6, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (7, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (8, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (9, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (10, 10.795648609686934),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (11, 10.7956484925566),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (12, 10.7956484925566),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (13, 10.795648492556602),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (14, 10.795648492556602),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (15, 10.795648492556602),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (16, 10.79564844470354),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (17, 10.79564844470354),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (18, 10.795648228264186),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (19, 10.795648228264186),\n",
      "\u001b[92mINFO \u001b[0m:      \t          (20, 10.795648276489962)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[36m(ClientAppActor pid=4908)\u001b[0m \u001b[93mWARNING \u001b[0m:   Manually terminating ClientAppActor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=14760)\u001b[0m [Client 1] Evaluate -> Loss: 7011442.5000, RMSE: 2647.9129\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "[FLExperiment] Federated training finished.\n"
     ]
    }
   ],
   "source": [
    "fl_exp = FLExperiment(\n",
    "    model_cls=EZNet,\n",
    "    client_loaders=loaders,\n",
    "    num_clients=5,\n",
    "    num_rounds=20,\n",
    "    local_epochs=40,\n",
    "    task_type = TaskType.REGRESSION\n",
    ")\n",
    "\n",
    "\n",
    "fl_exp.run(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 rmse: 2721.624882308361\n",
      "Client 0 rmse: 2721.624882308361\n"
     ]
    }
   ],
   "source": [
    "# Test the client model 0 on the corresponding test set\n",
    "\n",
    "clients = fl_exp.get_clients()\n",
    "model0_1 = Net().to(DEVICE)\n",
    "model0_1.load_state_dict(clients[0])\n",
    "model0_1.eval()\n",
    "\n",
    "_, _, test_loader_1 = loaders[0]\n",
    "loss, rmse = test_regression(model0_1, test_loader_1)\n",
    "print(f\"Client 0 rmse: {rmse}\")\n",
    "\n",
    "# Or Alternatively\n",
    "client_dls = fl_exp.get_client_dataloader_tuples()\n",
    "model0_2 = Net().to(DEVICE)\n",
    "model0_2.load_state_dict(client_dls[0][0])\n",
    "model0_2.eval()\n",
    "\n",
    "_, _, test_loader_2 = client_dls[0][1]\n",
    "loss, rmse = test_regression(model0_2, test_loader_2)\n",
    "print(f\"Client 0 rmse: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client_0 loss: 10418487.384615384, RMSE: 3227.7681739268983\n",
      "Client_1 loss: 8274908.235294118, RMSE: 2876.614022647828\n",
      "Client_2 loss: 5841198.285714285, RMSE: 2416.857109080776\n",
      "Client_3 loss: 5873791.555555556, RMSE: 2423.590632832937\n",
      "Client_4 loss: 6741237.777777778, RMSE: 2596.389373298577\n"
     ]
    }
   ],
   "source": [
    "clients = fl_exp.get_clients()\n",
    "for i, client in enumerate(clients):\n",
    "    model = Net().to(DEVICE)\n",
    "    model.load_state_dict(client)\n",
    "    model.eval()\n",
    "\n",
    "    _, _, test_loader = loaders[i]\n",
    "    loss, rmse = test_regression(model, test_loader)\n",
    "    print(f'Client_{i} loss: {loss}, RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client_0 loss: 10811748.923076924, RMSE: 3288.1224008660206\n",
      "Client_1 loss: 8768443.294117646, RMSE: 2961.1557362147714\n",
      "Client_2 loss: 5224697.142857143, RMSE: 2285.759642407124\n",
      "Client_3 loss: 4754843.111111111, RMSE: 2180.5602745879582\n",
      "Client_4 loss: 7806919.111111111, RMSE: 2794.086453764649\n"
     ]
    }
   ],
   "source": [
    "clients = fl_exp.get_clients()\n",
    "for i, client in enumerate(clients):\n",
    "    model = EZNet().to(DEVICE)\n",
    "    model.load_state_dict(client)\n",
    "    model.eval()\n",
    "\n",
    "    _, _, test_loader = loaders[i]\n",
    "    loss, rmse = test_regression(model, test_loader)\n",
    "    print(f'Client_{i} loss: {loss}, RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 167311660.6511628\n",
      "Epoch 2: train loss 163099177.6744186\n",
      "Epoch 3: train loss 164928924.27906978\n",
      "Epoch 4: train loss 163351947.90697673\n",
      "Epoch 5: train loss 158738045.02325583\n",
      "Epoch 6: train loss 155094438.69767442\n",
      "Epoch 7: train loss 152832568.55813953\n",
      "Epoch 8: train loss 156088040.1860465\n",
      "Epoch 9: train loss 154847607.06976745\n",
      "Epoch 10: train loss 151236158.5116279\n",
      "Epoch 11: train loss 151962135.8139535\n",
      "Epoch 12: train loss 150747857.8604651\n",
      "Epoch 13: train loss 147368954.04651162\n",
      "Epoch 14: train loss 146689687.8139535\n",
      "Epoch 15: train loss 146484649.6744186\n",
      "Epoch 16: train loss 142373102.1395349\n",
      "Epoch 17: train loss 141790279.44186047\n",
      "Epoch 18: train loss 141753927.44186047\n",
      "Epoch 19: train loss 140122600.1860465\n",
      "Epoch 20: train loss 136737369.30232558\n",
      "Epoch 21: train loss 137326321.11627907\n",
      "Epoch 22: train loss 137923643.5348837\n",
      "Epoch 23: train loss 133430926.88372093\n",
      "Epoch 24: train loss 131383587.72093023\n",
      "Epoch 25: train loss 134725776.37209302\n",
      "Epoch 26: train loss 132453831.44186047\n",
      "Epoch 27: train loss 133038781.02325581\n",
      "Epoch 28: train loss 130161799.44186047\n",
      "Epoch 29: train loss 126404405.58139534\n",
      "Epoch 30: train loss 127375062.3255814\n",
      "Epoch 31: train loss 123578519.81395349\n",
      "Epoch 32: train loss 127367537.11627907\n",
      "Epoch 33: train loss 124730832.37209302\n",
      "Epoch 34: train loss 120935614.51162791\n",
      "Epoch 35: train loss 120825410.97674419\n",
      "Epoch 36: train loss 119581526.3255814\n",
      "Epoch 37: train loss 119306808.55813953\n",
      "Epoch 38: train loss 118281988.46511628\n",
      "Epoch 39: train loss 120143057.86046511\n",
      "Epoch 40: train loss 117767924.09302326\n",
      "Epoch 41: train loss 114706344.18604651\n",
      "Epoch 42: train loss 113249416.93023255\n",
      "Epoch 43: train loss 113344254.51162791\n",
      "Epoch 44: train loss 112149931.1627907\n",
      "Epoch 45: train loss 111964180.8372093\n",
      "Epoch 46: train loss 108534138.04651164\n",
      "Epoch 47: train loss 108261108.09302326\n",
      "Epoch 48: train loss 109660881.86046511\n",
      "Epoch 49: train loss 108678952.18604651\n",
      "Epoch 50: train loss 102913357.39534883\n",
      "Epoch 51: train loss 103806254.13953489\n",
      "Epoch 52: train loss 103641555.34883721\n",
      "Epoch 53: train loss 103283901.02325581\n",
      "Epoch 54: train loss 101300869.95348836\n",
      "Epoch 55: train loss 104408132.46511628\n",
      "Epoch 56: train loss 103338180.46511628\n",
      "Epoch 57: train loss 100416695.06976745\n",
      "Epoch 58: train loss 102103464.18604651\n",
      "Epoch 59: train loss 97708765.76744185\n",
      "Epoch 60: train loss 96509343.25581396\n",
      "Epoch 61: train loss 98475686.69767442\n",
      "Epoch 62: train loss 95256878.13953489\n",
      "Epoch 63: train loss 92247688.93023255\n",
      "Epoch 64: train loss 96295995.53488372\n",
      "Epoch 65: train loss 92452800.0\n",
      "Epoch 66: train loss 91326547.34883721\n",
      "Epoch 67: train loss 93245816.55813953\n",
      "Epoch 68: train loss 92787530.41860466\n",
      "Epoch 69: train loss 90578210.23255815\n",
      "Epoch 70: train loss 89648077.39534883\n",
      "Epoch 71: train loss 88639175.44186047\n",
      "Epoch 72: train loss 89760634.79069768\n",
      "Epoch 73: train loss 85854158.88372093\n",
      "Epoch 74: train loss 88261805.39534883\n",
      "Epoch 75: train loss 83324347.53488372\n",
      "Epoch 76: train loss 83756231.44186047\n",
      "Epoch 77: train loss 84155853.39534883\n",
      "Epoch 78: train loss 83775086.13953489\n",
      "Epoch 79: train loss 84039421.02325581\n",
      "Epoch 80: train loss 83408055.81395349\n",
      "Epoch 81: train loss 80626464.74418604\n",
      "Epoch 82: train loss 83927868.27906977\n",
      "Epoch 83: train loss 79505550.88372093\n",
      "Epoch 84: train loss 81852606.51162791\n",
      "Epoch 85: train loss 79556610.97674419\n",
      "Epoch 86: train loss 80096215.81395349\n",
      "Epoch 87: train loss 75766163.34883721\n",
      "Epoch 88: train loss 80009529.30232558\n",
      "Epoch 89: train loss 77458976.0\n",
      "Epoch 90: train loss 72988256.74418604\n",
      "Epoch 91: train loss 72186580.8372093\n",
      "Epoch 92: train loss 74035345.86046511\n",
      "Epoch 93: train loss 74007072.74418604\n",
      "Epoch 94: train loss 74876292.46511628\n",
      "Epoch 95: train loss 70927462.69767442\n",
      "Epoch 96: train loss 71506573.39534883\n",
      "Epoch 97: train loss 71959608.55813953\n",
      "Epoch 98: train loss 69792245.58139534\n",
      "Epoch 99: train loss 69746682.04651164\n",
      "Epoch 100: train loss 72140210.60465117\n",
      "(1611347.6666666667, 1269.3886980222671)\n"
     ]
    }
   ],
   "source": [
    "eznet = EZNet()\n",
    "train(eznet, loaders[0][0], epochs=100, verbose=True, task_type=TaskType.REGRESSION)\n",
    "print(test_regression(eznet, loaders[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047]) tensor(0.0083)\n",
      "tensor([ 0.2779,  0.7342, -0.3736, -0.3952, -1.2449, -0.4260, -0.9261,  0.3349]) tensor(-3.0282)\n",
      "tensor([-0.9757, -1.3057,  1.8161,  1.0204, -0.5522,  0.5234,  0.4719, -0.2912]) tensor(1.0606)\n",
      "tensor([-0.6554, -0.1144, -1.0761, -0.3204,  0.0177, -0.8664, -0.1684, -1.2356]) tensor(-6.6284)\n",
      "Epoch 1: train loss 16.926950923374722\n",
      "Epoch 2: train loss 16.10869864327567\n",
      "Epoch 3: train loss 15.310064054216657\n",
      "Epoch 4: train loss 14.559073726109096\n",
      "Epoch 5: train loss 13.840103334699359\n",
      "Epoch 6: train loss 13.141866607666016\n",
      "Epoch 7: train loss 12.47857789175851\n",
      "Epoch 8: train loss 11.84029914855957\n",
      "Epoch 9: train loss 11.2280201285226\n",
      "Epoch 10: train loss 10.639702268327985\n",
      "Epoch 11: train loss 10.07326446533203\n",
      "Epoch 12: train loss 9.534879259381976\n",
      "Epoch 13: train loss 9.01893990107945\n",
      "Epoch 14: train loss 8.519811940874373\n",
      "Epoch 15: train loss 8.043475701468331\n",
      "Epoch 16: train loss 7.58717914036342\n",
      "Epoch 17: train loss 7.150384973798479\n",
      "Epoch 18: train loss 6.731913615635463\n",
      "Epoch 19: train loss 6.331510265895298\n",
      "Epoch 20: train loss 5.953755836486817\n",
      "(4.934424524307251, 2.2213564604329603)\n"
     ]
    }
   ],
   "source": [
    "def generate_random_dataset(n_samples=10000, n_features=5):\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    # Y = torch.zeros(n_samples) \n",
    "    Y = 1.5 * X.sum(dim=1)\n",
    "    print(X[0], Y[0])\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_random_client_loaders(\n",
    "    num_clients=4,\n",
    "    n_samples_per_client=1000,\n",
    "    n_features=8,\n",
    "    batch_size=16,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    seed=42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    client_loaders = []\n",
    "    for client_id in range(num_clients):\n",
    "        dataset = generate_random_dataset(n_samples_per_client, n_features)\n",
    "        total_len = len(dataset)\n",
    "        train_len = int(train_ratio * total_len)\n",
    "        val_len = int(val_ratio * total_len)\n",
    "        test_len = total_len - train_len - val_len\n",
    "\n",
    "        train_ds, val_ds, test_ds = random_split(\n",
    "            dataset,\n",
    "            lengths=[train_len, val_len, test_len],\n",
    "            generator=torch.Generator().manual_seed(seed + client_id)\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "        client_loaders.append((train_loader, val_loader, test_loader))\n",
    "\n",
    "    return client_loaders\n",
    "\n",
    "loaders_regression = create_random_client_loaders(num_clients=4)\n",
    "\n",
    "eznet = EZNet()\n",
    "train(eznet, loaders_regression[0][0], epochs=20, verbose=True, task_type=TaskType.REGRESSION)\n",
    "print(test_regression(eznet, loaders_regression[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
